version: 0.2

phases:
  install:
    runtime-versions:
      python: 3.8
    commands:
      - echo Installing dependencies
      - pip install --upgrade pip
      - pip install awscli boto3 sagemaker
  
  pre_build:
    commands:
      # Setup environment variables
      - ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
      - PROJECT_NAME=$(cat cloudformation.yaml | grep -m 1 'ProjectName' -A 3 | grep 'Default:' | awk '{print $2}')
      - ENVIRONMENT=$(cat cloudformation.yaml | grep -m 1 'EnvironmentName' -A 3 | grep 'Default:' | awk '{print $2}')
      
      # Create S3 bucket for model artifacts if it doesn't exist
      - S3_BUCKET="${PROJECT_NAME}-${ENVIRONMENT}-artifacts-${ACCOUNT_ID}"
      - S3_DATA_PREFIX="mscoco_processed"
      - aws s3api head-bucket --bucket $S3_BUCKET 2>/dev/null || aws s3 mb s3://$S3_BUCKET
      
      # Check preprocessing flag
      - |
        UPLOAD_DATA=$(aws ssm get-parameter --name "/${PROJECT_NAME}/${ENVIRONMENT}/upload-data" --query "Parameter.Value" --output text || echo "false")
        echo "Upload data: $UPLOAD_DATA"
  
  build:
    commands:
      # Install specific dependencies for data processing
      - echo "Checking script files..."
      - ls -la scripts/
      # Conditionally process and upload data
      - |
        if [ "$UPLOAD_DATA" = "true" ]; then
          echo "Launching SageMaker Processing Job for MS-COCO dataset processing..."
          
          # Get SageMaker role ARN
          SAGEMAKER_ROLE_ARN=$(aws cloudformation describe-stacks --stack-name STACKMOEGAN --query "Stacks[0].Outputs[?OutputKey=='SageMakerRoleArn'].OutputValue" --output text)
          
          # If the ARN retrieval fails, use a hardcoded backup
          if [ -z "$SAGEMAKER_ROLE_ARN" ]; then
            echo "Failed to get SageMaker role ARN from CloudFormation, using default"
            SAGEMAKER_ROLE_ARN="arn:aws:iam::207567761455:role/${PROJECT_NAME}-${ENVIRONMENT}-sagemaker-role"
          fi
          
          echo "Using SageMaker role ARN: $SAGEMAKER_ROLE_ARN"
          
          # Launch processing job with GitHub repository URL
          GITHUB_REPO="https://github.com/shmurthy08/MoE-GAN_CPSC541"
          python scripts/launch_processing_job.py \
            --role-arn $SAGEMAKER_ROLE_ARN \
            --bucket $S3_BUCKET \
            --prefix $S3_DATA_PREFIX \
            --max-samples -1 \
            --instance-type ml.m5.4xlarge \
            --volume-size 500
          
          # Update parameter store to avoid reprocessing next time
          aws ssm put-parameter --name "/${PROJECT_NAME}/${ENVIRONMENT}/upload-data" --type "String" --value "false" --overwrite
          
          echo "SageMaker Processing Job launched. Data will be uploaded to S3: s3://${S3_BUCKET}/${S3_DATA_PREFIX}/"
        else
          echo "Skipping data processing. To process data, set the SSM parameter /${PROJECT_NAME}/${ENVIRONMENT}/upload-data to 'true'"
        fi
  
  post_build:
    commands:
      # Create output file
      - |
        cat > preprocessing_output.json << EOF
        {
          "S3Bucket":"$S3_BUCKET",
          "S3DataPrefix":"$S3_DATA_PREFIX",
          "ProcessingCompleted": "$UPLOAD_DATA"
        }
        EOF
      - echo "Preprocessing stage complete."

artifacts:
  files:
    - preprocessing_output.json
  discard-paths: yes