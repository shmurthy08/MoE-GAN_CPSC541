version: 0.2

phases:
  install:
    runtime-versions:
      python: 3.8
    commands:
      - echo Installing dependencies
      - pip install --upgrade pip
      - pip install awscli boto3 sagemaker
  
  pre_build:
    commands:
      # Setup environment variables
      - ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
      - PROJECT_NAME=$(cat cloudformation.yaml | grep -m 1 'ProjectName' -A 3 | grep 'Default:' | awk '{print $2}')
      - ENVIRONMENT=$(cat cloudformation.yaml | grep -m 1 'EnvironmentName' -A 3 | grep 'Default:' | awk '{print $2}')
      
      # Create S3 bucket for model artifacts if it doesn't exist
      - S3_BUCKET="${PROJECT_NAME}-${ENVIRONMENT}-artifacts-${ACCOUNT_ID}"
      - S3_DATA_PREFIX="mscoco_processed"
      - aws s3api head-bucket --bucket $S3_BUCKET 2>/dev/null || aws s3 mb s3://$S3_BUCKET
      
      # Check preprocessing flag
      - |
        UPLOAD_DATA=$(aws ssm get-parameter --name "/moe-gan/${ENVIRONMENT}/upload-data" --query "Parameter.Value" --output text || echo "false")
        echo "Upload data: $UPLOAD_DATA"
  
  build:
    commands:
      # Install specific dependencies for data processing
      - echo "Checking script files..."
      - ls -la scripts/
      # Conditionally process and upload data
      - |
        if [ "$UPLOAD_DATA" = "true" ]; then
          echo "Creating SageMaker Processing Job for data processing..."
          # Install dependencies for launching the job
          pip install boto3 sagemaker
          
          # Copy processing script to scripts directory
          cp processing_script.py scripts/
          
          # Launch processing job
          SAGEMAKER_ROLE_ARN=$(aws cloudformation describe-stacks --stack-name ${PROJECT_NAME}-${ENVIRONMENT} --query "Stacks[0].Outputs[?OutputKey=='SageMakerRoleArn'].OutputValue" --output text || echo "arn:aws:iam::${ACCOUNT_ID}:role/${PROJECT_NAME}-${ENVIRONMENT}-sagemaker-role")
          
          python scripts/launch_processing_job.py \
            --role-arn $SAGEMAKER_ROLE_ARN \
            --bucket $S3_BUCKET \
            --prefix $S3_DATA_PREFIX \
            --max-samples -1 \
            --instance-type ml.m5.4xlarge \
            --volume-size 300
          
          echo "SageMaker Processing Job launched. Data will be uploaded to S3: s3://${S3_BUCKET}/${S3_DATA_PREFIX}/"
        else
          echo "Skipping data processing. Use [upload-data] in commit message to trigger."
        fi
  
  post_build:
    commands:
      # Create output file
      - |
        cat > preprocessing_output.json << EOF
        {
          "S3Bucket":"$S3_BUCKET",
          "S3DataPrefix":"$S3_DATA_PREFIX",
          "ProcessingCompleted": "$UPLOAD_DATA"
        }
        EOF
      - echo "Preprocessing stage complete."

artifacts:
  files:
    - preprocessing_output.json
  discard-paths: yes